* Tasks
** v0.0.1: First, Simple version
*** [x] v0.0.1: Simple text scraper
*** [x] v0.0.1: Enable for all sites
*** [x] v0.0.1: On each page load, send scraped text to flask api
*** [x] v0.0.1: UI: Search for websites
** v0.0.2: Beautify UI, show decently structured results
*** [x] v0.0.2: UI: Beautify the UI, Show keywords cleanly, show search button, results cleanly etc
*** [x] v0.0.2: UI: Show results with numbering, make it clickable etc
*** [x] v0.0.2: SR: Remove the load and save to outside, should save before crash/exit
** v0.0.3: Refactored version, with accurate results, metrics, dataset etc
*** [.] v0.0.3: MJ: Refactor Code Base
**** [x] v0.0.3: AL: Refactor search, separate function to handle search with embedding
**** [x] v0.0.3: SR: Add class struct, init with test data from test and real data from server.py
                    - Handle all in clean formats, train, test, production
**** [x] v0.0.3: SR: Save extracted data on disk
**** [x] v0.0.3: SR: Save extracted data, chunked data every 100 or so iters
**** [x] v0.0.3: AL: Add 1000 urls, save both train and test data as embeddings, test code
**** [x] v0.0.3: AL: Fix paths for relative imports
**** [x] v0.0.3: AL: Save crawled url text
**** [x] v0.0.3: AL: Fix code to have accurate results (1100)
**** [x] v0.0.3: AL: Move test code into separate test_algo.py
**** [x] v0.0.3: AL: Complete production use case code and test thoroughly
**** [x] v0.0.3: AL: Move production code to server.py, test remaining
**** [x] v0.0.3: AL: Remove deadcode and files (first-task)
**** [x] v0.0.3: SR: Document start, update requirements & test once (first-task)
**** [ ] v0.0.3: SR: Remove duplicate URLS (first-task)
**** [ ] v0.0.3: SR: AtExit does not seem to work if you trap signals (first-task)
          https://stackoverflow.com/questions/40866576/run-atexit-when-python-process-is-killed)
**** [ ] v0.0.3: SR: Store and send back page title (first-task)
***** [ ] v0.0.3: BE: Capture page title & store in database, update API to return same
***** [ ] v0.0.3: UI: Show page title, truncate when too large and show ...
**** [ ] v0.0.3: SR: Datetime handle (first-task)
***** [ ] v0.0.3: UI: Send datetime as well, handle same in BE
***** [ ] v0.0.x: SR: Store datetime and return with datetime
*** [ ] v0.0.3: SW: Release plan
**** [ ] v0.0.3: SW: Chrome Plugin + python package (autoinstall on linux)
**** [ ] v0.0.3: SW: Chrome Plugin + briefcase app
**** [ ] v0.0.3: SW: Simple to install plugin + pip like install wheel file or share executable
*** [x] v0.0.3: AL: Error Categorization: Top 5 is 90%, 100 GT, 1000 Confusables, so paused
**** [x] v0.0.3: AL: Metrics: Top 5 Errors, Multicategory error
**** [ ] v0.0.3: AL: Error characterization for 100 test urls
*** [x] v0.0.3: SW: UI slow: needs async indexing
*** [.] v0.0.3: SW: Issue with index not found (debug the same)
**** [x] Issue not there in clean meta/embed data (like 1000k training set)
**** [ ] Might be issue when exit happens uncleanly
** v0.0.4: Export History, Experimental Other Sources, Experimental Fast HN
*** [ ] v0.0.4: UI: Create FF plugin
*** [ ] v0.0.4: UI: Only save read text data
*** [ ] v0.0.4: UI: Move from popup to a full page plugin with button for export history
*** [ ] v0.0.4: AL: Improve 'find' speed further, should respond E2E in 200-300ms
*** [ ] v0.0.4: AL: Improve 'index' speed further, now takes 10hrs for 1000 urls
*** [ ] v0.0.4: AL: Improve 'index' space requirement, now takes 100MB for 20MB text
*** [ ] v0.0.4: PD: Tabmanger Export (optional)
**** [ ] v0.0.4: UI: TabManager Export Button + Tab manager export to API
*** [ ] v0.0.4: PD: HistoryExport Button + History export to API
**** [ ] v0.0.4: UI: Export History button
**** [ ] v0.0.4: BE: API to handle + running long running task
*** [ ] v0.0.4: PD: Manage Other Sources (Experimental)
**** [ ] v0.0.4: EX: Export Google Drive or Confluence (check out how llamaindex etc manages this)
*** [ ] v0.0.4: PD: HN Scalable (upto 100 req/s on single server, Experimental)
**** [ ] v0.0.4: EX: Scalable version exploration (150 new pages)
*** [ ] v0.0.4: PD: Show Browsing Clusters: Showcase clusters of what all you see
**** [ ] v0.0.4: PD: Basic cluster view
**** [ ] v0.0.4: PD: Show timeline view
** Bucket List
*** [ ] v0.0.4: BE: Use pydantic across board for better data usage (TBD: Decide if necessary)
*** [ ] v0.0.x: PD: Privacy, mark out private communications separately
*** [ ] v0.0.x: PD: Ability to 
*** [ ] v0.0.x: PD: Ability to group website from clusters and make notes easily (use summary to provide grouping)
*** [ ] v0.0.x: BE: Common site specific parsers, to clean input (reddit, hnews, Goog etc)
*** [ ] v0.0.x: BE: Add HN parser (remove extraneous stuff)
*** [ ] v0.0.x: BE: Add Confluence parser
*** [ ] v0.0.x: AL: Reduce space usage, optimize the chunking size (optimal chunk size: rsch project)
*** [ ] v0.0.x: AL: Knowledge Representation (Major Product)
